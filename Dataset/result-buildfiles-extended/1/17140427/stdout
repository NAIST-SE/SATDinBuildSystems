{
  "Repository" : "17140427",
  "Revision" : "HEAD",
  "ObjectId" : "204685865147d0a6cea6148295def1ac9ba2decb",
  "CommitTime" : "2019-04-16T15:16:09Z",
  "Files" : {
    "robots.txt.pro" : {
      "ObjectId" : "14f77c49bf2d6cbb3178f0439702cbd56504c6bf",
      "LastModified" : "2016-12-03T12:01:21Z",
      "FileType" : "QMAKE",
      "0" : {
        "Text" : "#\n\n# robots.txt\n\n#\n\n# This file is to prevent the crawling and indexing of certain parts\n\n# of your site by web crawlers and spiders run by sites like Yahoo!\n\n# and Google. By telling these \"robots\" where not to go on your site,\n\n# you save bandwidth and server resources.\n\n#\n\n# This file will be ignored unless it is at the root of your host:\n\n# Used:    http://example.com/robots.txt\n\n# Ignored: http://example.com/site/robots.txt\n\n#\n\n# For more information about the robots.txt standard, see:\n\n# http://www.robotstxt.org/robotstxt.html\n\n#\n\n# For syntax checking, see:\n\n# http://www.frobee.com/robots-txt-check\n",
        "Line" : 1,
        "CharPositionInLine" : 0
      },
      "1" : {
        "Text" : "# Directories\n",
        "Line" : 21,
        "CharPositionInLine" : 0
      },
      "2" : {
        "Text" : "# Files\n",
        "Line" : 28,
        "CharPositionInLine" : 0
      },
      "3" : {
        "Text" : "# Paths (clean URLs)\n",
        "Line" : 41,
        "CharPositionInLine" : 0
      },
      "4" : {
        "Text" : "# Paths (no clean URLs)\n",
        "Line" : 51,
        "CharPositionInLine" : 0
      },
      "5" : {
        "Text" : "# Other\n",
        "Line" : 61,
        "CharPositionInLine" : 0
      },
      "CommentCount" : 6
    }
  },
  "FileTypes" : {
    "QMAKE" : 1
  },
  "ElapsedTime" : 1488
}